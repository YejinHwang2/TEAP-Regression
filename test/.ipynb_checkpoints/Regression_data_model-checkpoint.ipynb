{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c4d92ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Generator\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torchmetrics as tm\n",
    "main_path = Path('..').resolve()\n",
    "sys.path.append(str(main_path))\n",
    "from copy import deepcopy\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba9064c",
   "metadata": {},
   "source": [
    "## Data & Task generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e15258fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(li: List[Any]) -> Generator:\n",
    "    \"\"\"flatten nested list\n",
    "    ```python\n",
    "    x = [[[1], 2], [[[[3]], 4, 5], 6], 7, [[8]], [9], 10]\n",
    "    print(type(flatten(x)))\n",
    "    # <generator object flatten at 0x00000212BF603CC8>\n",
    "    print(list(flatten(x)))\n",
    "    # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    ```\n",
    "    Args:\n",
    "        li (List[Any]): any kinds of list\n",
    "    Yields:\n",
    "        Generator: flattened list generator\n",
    "    \"\"\"\n",
    "    for ele in li:\n",
    "        if isinstance(ele, list) or isinstance(ele, tuple):\n",
    "            yield from flatten(ele)\n",
    "        else:\n",
    "            yield ele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9c6117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PanelDataDict(dict):\n",
    "    def __init__(self, data, window_size):\n",
    "        self.window_size = window_size\n",
    "        self._set_state(f'numpy')\n",
    "        for k, v in data.items():\n",
    "            data[k] = np.array(v)\n",
    "        \n",
    "        self.n_stocks = len(v)\n",
    "        super().__init__(data)\n",
    "    \n",
    "    def tensor_fn(self, value, key):\n",
    "        return torch.FloatTensor(value)\n",
    "\n",
    "    def _set_state(self, state: str):\n",
    "        self.state = state\n",
    "\n",
    "    def to(self, device: None | str=None):\n",
    "        if device is None:\n",
    "            device = torch.device('cpu')\n",
    "        else:\n",
    "            device = torch.device(device)\n",
    "        self._set_state(f'tensor.{device}')\n",
    "        for key in self.keys():\n",
    "            value = self.__getitem__(key)\n",
    "            tvalue = self.tensor_fn(value, key)\n",
    "            self.__setitem__(key, tvalue.to(device)) \n",
    "        \n",
    "    def numpy(self):\n",
    "        self._set_state('numpy')\n",
    "        for key in self.keys():\n",
    "            tvalue = self.__getitem__(key)\n",
    "            if not isinstance(tvalue, np.ndarray): \n",
    "                self.__setitem__(key, tvalue.detach().numpy())\n",
    "\n",
    "    def __str__(self):\n",
    "        s = f'PanelDataDict(T={self.window_size}, {self.state})\\n'\n",
    "        for i, key in enumerate(self.keys()):\n",
    "            value = self.__getitem__(key)\n",
    "            s += f'- {key}: {value.shape}'\n",
    "            s += '' if i == len(self.keys())-1 else '\\n'\n",
    "        return s\n",
    "\n",
    "class StockRegressionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            meta_type: str ='train', \n",
    "            data_dir: Path | str ='', \n",
    "            dtype: str ='kdd17', \n",
    "            batch_size: int =64,\n",
    "            n_support: int =5, \n",
    "            n_query: int = 3,\n",
    "            window_sizes: List[int] =[5]\n",
    "        ):    \n",
    "        \"\"\"dataset ref: https://arxiv.org/abs/1810.09936\n",
    "\n",
    "        In this meta learning setting, we have 3 meta-test and 1 meta-train\n",
    "        vertical = stocks, horizontal = time\n",
    "                train      |    test\n",
    "           A               |\n",
    "           B   meta-train  |   meta-test\n",
    "           C               |      (1)\n",
    "           ----------------|-------------\n",
    "           D   meta-test   |   meta-test\n",
    "           E     (2)       |      (3)\n",
    "\n",
    "        meta-test (1) same stock, different time\n",
    "        meta-test (2) different stock, same time\n",
    "        meta-test (3) different stock, different time\n",
    "        use `valid_date` to split the train / test set\n",
    "\n",
    "        the number of training stock was splitted with number of total stocks * 0.8.\n",
    "        we have 5 stock universe\n",
    "\n",
    "        Args:\n",
    "            meta_type (str, optional): _description_. Defaults to 'train'.\n",
    "            data_dir (Path | str, optional): _description_. Defaults to ''.\n",
    "            dtype (str, optional): _description_. Defaults to 'kdd17'.\n",
    "            stock_universe (int, optional): _description_. Defaults to 0.\n",
    "            batch_size (int, optional): Batch size. Number of stock x Number of timestamp that is aviable for each window size. Defaults to 64.\n",
    "            n_support (int, optional): Number of support. Defaults to 4.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # data config\n",
    "        self.data_dir = Path(data_dir).resolve()\n",
    "        ds_info = {\n",
    "            # train: (Jan-01-2007 to Jan-01-2015)\n",
    "            # val: (Jan-01-2015 to Jan-01-2016)\n",
    "            # test: (Jan-01-2016 to Jan-01-2017)\n",
    "            'kdd17': {\n",
    "                'path': self.data_dir / 'kdd17/price_long_50',\n",
    "                'date': self.data_dir / 'kdd17/trading_dates.csv',\n",
    "                'universe': self.data_dir / 'kdd17/stock_universe.json', \n",
    "                'start_date': '2007-01-01',\n",
    "                'train_date': '2015-01-01', \n",
    "                'valid_date': '2016-01-01', \n",
    "                'test_date': '2017-01-01',\n",
    "            },\n",
    "            # train: (Jan-01-2014 to Aug-01-2015)\n",
    "            # val: (Aug-01-2015 to Oct-01-2015)\n",
    "            # test: (Oct-01-2015 to Jan-01-2016)\n",
    "            'acl18': {\n",
    "                'path': self.data_dir / 'stocknet-dataset/price/raw',\n",
    "                'date': self.data_dir / 'stocknet-dataset/price/trading_dates.csv',\n",
    "                'universe': self.data_dir / 'stocknet-dataset/stock_universe.json',\n",
    "                'start_date': '2014-01-01',\n",
    "                'train_date': '2015-08-01', \n",
    "                'valid_date': '2015-10-01', \n",
    "                'test_date': '2016-01-01',\n",
    "            }\n",
    "        }\n",
    "        ds_config = ds_info[dtype]\n",
    "        \n",
    "        self.meta_type = meta_type\n",
    "        self.window_sizes = window_sizes\n",
    "        self.batch_size = batch_size\n",
    "        self.n_support = n_support\n",
    "        self.n_query = n_query\n",
    "\n",
    "        # get data\n",
    "        self.data = {}\n",
    "        self.all_tasks = {}\n",
    "        ps = list((ds_config['path']).glob('*.csv'))\n",
    "        with ds_config['universe'].open('r') as file:\n",
    "            universe_dict = json.load(file)\n",
    "        \n",
    "        # meta_type: train / valid1: valid-time, valid2: valid-stock, valid3: valid-mix / test1, test2, test3\n",
    "        if meta_type in ['train', 'valid-time', 'test-time']:\n",
    "            universe = universe_dict['train']\n",
    "        elif meta_type in ['valid-stock', 'valid-mix']:\n",
    "            universe = universe_dict['valid']\n",
    "        elif meta_type in ['test-stock', 'test-mix']:\n",
    "            universe = universe_dict['test']\n",
    "        else:\n",
    "            raise KeyError('Error argument `meta_type`, should be in (train, valid-time, valid-stock, valid-mix, test-time, test-stock, test-mix)')\n",
    "\n",
    "        if meta_type in ['train', 'valid-stock', 'test-stock']:\n",
    "            date1 = ds_config['start_date']\n",
    "            date2 = ds_config['train_date']\n",
    "        elif meta_type in ['valid-time', 'valid-mix']:\n",
    "            date1 = ds_config['train_date']\n",
    "            date2 = ds_config['valid_date']\n",
    "        elif meta_type in ['test-time', 'test-mix']:\n",
    "            date1 = ds_config['valid_date']\n",
    "            date2 = ds_config['test_date']\n",
    "        else:\n",
    "            raise KeyError('Error argument `meta_type`, should be in (train, valid-time, valid-stock, valid-mix, test-time, test-stock, test-mix)')\n",
    "\n",
    "        iterator = [p for p in ps if p.name.strip('.csv') in universe]\n",
    "        for p in tqdm(iterator, total=len(iterator), desc=f'Processing data for {self.meta_type}'):    \n",
    "            stock_symbol = p.name.rstrip('.csv')\n",
    "            df_single = self.load_single_stock(p)\n",
    "            cond = df_single['date'].between(date1, date2)\n",
    "            df_single = df_single.loc[cond].reset_index(drop=True)\n",
    "            \n",
    "            self.data[stock_symbol] = df_single\n",
    "\n",
    "\n",
    "        self.n_stocks = len(universe)\n",
    "\n",
    "\n",
    "    def load_single_stock(self, p: Path | str):\n",
    "        def longterm_trend(x: pd.Series, k:int):\n",
    "            return (x.rolling(k).sum().div(k*x) - 1) * 100\n",
    "\n",
    "        df = pd.read_csv(p)\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df = df.sort_values('Date').reset_index(drop=True)\n",
    "        if 'Unnamed' in df.columns:\n",
    "            df.drop(columns=df.columns[7], inplace=True)\n",
    "        if 'Original_Open' in df.columns:\n",
    "            df.rename(columns={'Original_Open': 'Open', 'Open': 'Adj Open'}, inplace=True)\n",
    "\n",
    "        # Open, High, Low\n",
    "        z1 = (df.loc[:, ['Open', 'High', 'Low']].div(df['Close'], axis=0) - 1).rename(\n",
    "            columns={'Open': 'open', 'High': 'high', 'Low': 'low'}) * 100\n",
    "        # Close\n",
    "        z2 = df[['Close']].pct_change().rename(columns={'Close': 'close'}) * 100\n",
    "        # Adj Close\n",
    "        z3 = df[['Adj Close']].pct_change().rename(columns={'Adj Close': 'adj_close'}) * 100\n",
    "\n",
    "        z4 = []\n",
    "        for k in [5, 10, 15, 20, 25, 30]:\n",
    "            z4.append(df[['Adj Close']].apply(longterm_trend, k=k).rename(columns={'Adj Close': f'zd{k}'}))\n",
    "\n",
    "        df_pct = pd.concat([df['Date'], z1, z2, z3] + z4, axis=1).rename(columns={'Date': 'date'})\n",
    "        cols_max = df_pct.columns[df_pct.isnull().sum() == df_pct.isnull().sum().max()]\n",
    "        df_pct = df_pct.loc[~df_pct[cols_max].isnull().values, :]\n",
    "\n",
    "        return df_pct\n",
    "\n",
    "    def sliding_window_idx(self, df_single, window_size):\n",
    "    \n",
    "        if len(df_single) >= window_size:\n",
    "            x_spt_task = []\n",
    "            y_spt_task = []\n",
    "            x_qry_task = []\n",
    "            y_qry_task = []\n",
    "\n",
    "            for i in range(len(df_single)-window_size-self.n_support-self.n_query+1):\n",
    "                x_spt = []\n",
    "                y_spt = []\n",
    "                x_qry = []\n",
    "                y_qry = []\n",
    "\n",
    "                for j in range(self.n_support+self.n_query):\n",
    "                    if j < self.n_support:\n",
    "                        spt_idx = [idx for idx in range(i+j, i+j+window_size)]\n",
    "                        x_spt.append(spt_idx)\n",
    "                        y_spt.append(i+j+window_size)\n",
    "\n",
    "                    else:\n",
    "                        qry_idx = [idx for idx in range(i+j, i+j+window_size)]\n",
    "                        x_qry.append(qry_idx)\n",
    "                        y_qry.append(i+j+window_size)\n",
    "\n",
    "                x_spt_task.append(x_spt)\n",
    "                y_spt_task.append(y_spt)\n",
    "                x_qry_task.append(x_qry)\n",
    "                y_qry_task.append(y_qry)\n",
    "            return x_spt_task, y_spt_task, x_qry_task, y_qry_task\n",
    "    \n",
    "    def generate_data(self,df_single, x_spt_task, y_spt_task, x_qry_task, y_qry_task):\n",
    "        num_task = len(x_spt_task)\n",
    "        support_task = []\n",
    "        support_labels = []\n",
    "        query_task = []\n",
    "        query_labels = []\n",
    "        for i in range(num_task):\n",
    "            support_inputs = []\n",
    "            query_inputs = []\n",
    "            for j in range(self.n_support):\n",
    "                support_inputs.append(df_single.iloc[x_spt_task[i][j]].to_numpy()[:, 1:].astype(np.float64))\n",
    "\n",
    "            support_labels.append(df_single['close'].iloc[y_spt_task[i]].to_numpy().astype(np.float64))\n",
    "            support_task.append(np.array(support_inputs))\n",
    "            for k in range(self.n_query):\n",
    "                query_inputs.append(df_single.iloc[x_qry_task[i][k]].to_numpy()[:, 1:].astype(np.float64))\n",
    "            query_labels.append(df_single['close'].iloc[y_qry_task[i]].to_numpy().astype(np.float64))\n",
    "            query_task.append(np.array(query_inputs))   \n",
    "\n",
    "        return support_task, support_labels, query_task, query_labels\n",
    "    \n",
    "    @property\n",
    "    def symbols(self):\n",
    "        return list(self.data.keys())\n",
    "    \n",
    "    def generate_all_task(self):\n",
    "        all_tasks = dict()\n",
    "        for window in self.window_sizes:\n",
    "            all_tasks[window] = self.generate_all_task_per_window(window)\n",
    "        self.all_tasks = all_tasks\n",
    "\n",
    "    def generate_all_task_per_window(self,window_size):\n",
    "        \n",
    "        all_window_tasks = dict(\n",
    "                query = [],\n",
    "                query_labels = [],\n",
    "                support = [],\n",
    "                support_labels = [],\n",
    "            )\n",
    "        for symbol in self.symbols:\n",
    "            df = self.data[symbol]\n",
    "            x_spt_task, y_spt_task, x_qry_task, y_qry_task = self.sliding_window_idx(df, window_size)\n",
    "            support_inputs, support_labels, query_inputs, query_labels = self.generate_data(df, x_spt_task, y_spt_task, x_qry_task, y_qry_task)\n",
    "            all_window_tasks['query'].extend(query_inputs)\n",
    "            all_window_tasks['query_labels'].extend(query_labels)\n",
    "            all_window_tasks['support'].extend(support_inputs)\n",
    "            all_window_tasks['support_labels'].extend(support_labels)\n",
    "        \n",
    "        all_window_tasks['query'] = np.array(all_window_tasks['query'])\n",
    "        all_window_tasks['query_labels'] = np.array(all_window_tasks['query_labels'])\n",
    "        all_window_tasks['support'] = np.array(all_window_tasks['support'])\n",
    "        all_window_tasks['support_labels'] = np.array(all_window_tasks['support_labels'])\n",
    "        return all_window_tasks\n",
    "    \n",
    "    def generate_batch_task(self, all_tasks):\n",
    "        batch_tasks = dict(\n",
    "                query = [],\n",
    "                query_labels = [],\n",
    "                support = [],\n",
    "                support_labels = [],\n",
    "            )\n",
    "\n",
    "        \n",
    "        if len(self.window_sizes) > 1:\n",
    "            window_size = random.choice(self.window_sizes)\n",
    "        else:\n",
    "            window_size = self.window_sizes[0]\n",
    "               \n",
    "        num_task = len(all_tasks[window_size]['query'])\n",
    "        batch_idx = random.sample(list(range(num_task)), self.batch_size)\n",
    "        batch_tasks['query'] = all_tasks[window_size]['query'][batch_idx]\n",
    "        batch_tasks['query_labels'] = all_tasks[window_size]['query_labels'][batch_idx]\n",
    "        batch_tasks['support'] = all_tasks[window_size]['support'][batch_idx]\n",
    "        batch_tasks['support_labels'] = all_tasks[window_size]['support_labels'][batch_idx]\n",
    "\n",
    "        return batch_tasks, window_size\n",
    "\n",
    "    def update_q_idx_dist(self, q_target):\n",
    "        self.q_dist[q_target] += 1\n",
    "\n",
    "    def reset_q_idx_dist(self):\n",
    "        self.q_dist = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84e138a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"C:/Users\\/david/yjhwang/TEAP/data\"\n",
    "dtype = \"kdd17\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d907923",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(data_dir).resolve()\n",
    "ds_info = {\n",
    "    # train: (Jan-01-2007 to Jan-01-2015)\n",
    "    # val: (Jan-01-2015 to Jan-01-2016)\n",
    "    # test: (Jan-01-2016 to Jan-01-2017)\n",
    "    'kdd17': {\n",
    "        'path': data_dir / 'kdd17/price_long_50',\n",
    "        'date': data_dir / 'kdd17/trading_dates.csv',\n",
    "        'universe': data_dir / 'kdd17/stock_universe.json', \n",
    "        'start_date': '2007-01-01',\n",
    "        'train_date': '2015-01-01', \n",
    "        'valid_date': '2016-01-01', \n",
    "        'test_date': '2017-01-01',\n",
    "    },\n",
    "    # train: (Jan-01-2014 to Aug-01-2015)\n",
    "    # val: (Aug-01-2015 to Oct-01-2015)\n",
    "    # test: (Oct-01-2015 to Jan-01-2016)\n",
    "    'acl18': {\n",
    "        'path': data_dir / 'stocknet-dataset/price/raw',\n",
    "        'date': data_dir / 'stocknet-dataset/price/trading_dates.csv',\n",
    "        'universe': data_dir / 'stocknet-dataset/stock_universe.json',\n",
    "        'start_date': '2014-01-01',\n",
    "        'train_date': '2015-08-01', \n",
    "        'valid_date': '2015-10-01', \n",
    "        'test_date': '2016-01-01',\n",
    "    }\n",
    "}\n",
    "ds_config = ds_info[dtype]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3777f91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data for train: 100%|██████████████████████████████████████████████████████████| 35/35 [00:02<00:00, 16.45it/s]\n",
      "Processing data for valid-time: 100%|█████████████████████████████████████████████████████| 35/35 [00:02<00:00, 15.84it/s]\n",
      "Processing data for valid-stock: 100%|████████████████████████████████████████████████████| 10/10 [00:00<00:00, 29.70it/s]\n",
      "Processing data for valid-mix: 100%|██████████████████████████████████████████████████████| 10/10 [00:00<00:00, 40.84it/s]\n",
      "Processing data for test-time: 100%|██████████████████████████████████████████████████████| 35/35 [00:02<00:00, 15.71it/s]\n",
      "Processing data for test-stock: 100%|███████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.80it/s]\n",
      "Processing data for test-mix: 100%|█████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 13.14it/s]\n"
     ]
    }
   ],
   "source": [
    "meta_train = StockRegressionDataset(data_dir = data_dir, n_query = 1)\n",
    "meta_valid_time = StockRegressionDataset(meta_type='valid-time', data_dir = data_dir, n_query = 1)\n",
    "meta_valid_entity = StockRegressionDataset(meta_type='valid-stock', data_dir = data_dir, n_query = 1)\n",
    "meta_valid_mix = StockRegressionDataset(meta_type='valid-mix', data_dir = data_dir, n_query = 1)\n",
    "meta_test_time = StockRegressionDataset(meta_type='test-time', data_dir = data_dir, n_query = 1)\n",
    "meta_test_entity = StockRegressionDataset(meta_type='test-stock', data_dir = data_dir, n_query = 1)\n",
    "meta_test_mix = StockRegressionDataset(meta_type='test-mix', data_dir = data_dir, n_query = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d71c052",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train.generate_all_task()\n",
    "meta_valid_time.generate_all_task()\n",
    "meta_valid_entity.generate_all_task()\n",
    "meta_valid_mix.generate_all_task()\n",
    "meta_test_time.generate_all_task()\n",
    "meta_test_entity.generate_all_task()\n",
    "meta_test_mix.generate_all_task()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9334548",
   "metadata": {},
   "source": [
    "## MetricRecorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a88d7c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionMetricRecorder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        cs = tm.MetricCollection({\n",
    "            'Support_MSE': tm.MeanMetric(), \n",
    "            'Support_MAE': tm.MeanMetric(),\n",
    "            'Support_MAPE': tm.MeanMetric(),\n",
    "            'Query_MSE': tm.MeanMetric(), \n",
    "            'Query_MAE': tm.MeanMetric(),\n",
    "            'Query_MAPE': tm.MeanMetric(),\n",
    "        })\n",
    "\n",
    "        self.metrics = cs.clone()\n",
    "    @property\n",
    "    def keys(self):\n",
    "        return list(self.metrics.keys())\n",
    "    \n",
    "    def update(self, key, scores):\n",
    "        self.metrics[key].update(scores)\n",
    "            \n",
    "    def compute(self, prefix: str):\n",
    "        results = {}\n",
    "        for k in self.keys:\n",
    "            m = self.metrics[k].compute()\n",
    "            if isinstance(m, torch.Tensor):\n",
    "                m = m.cpu().detach().numpy()\n",
    "            results[f'{prefix}-{k}'] = m\n",
    "        return results\n",
    "    \n",
    "    def reset(self):\n",
    "        for k in self.keys:\n",
    "            self.metrics[k].reset()\n",
    "            \n",
    "class RegressionMetricTaskRecorder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        cs = tm.MetricCollection({\n",
    "            'Support_MSE': tm.MeanSquaredError(), \n",
    "            'Support_MAE': tm.MeanAbsoluteError(),\n",
    "            'Support_MAPE': tm.MeanAbsolutePercentageError(),\n",
    "            'Query_MSE': tm.MeanSquaredError(), \n",
    "            'Query_MAE': tm.MeanAbsoluteError(),\n",
    "            'Query_MAPE': tm.MeanAbsolutePercentageError(),\n",
    "        })\n",
    "        \n",
    "        self.metrics = cs.clone()\n",
    "        \n",
    "    @property\n",
    "    def keys(self):\n",
    "        return list(self.metrics.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bfc4da",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0865818",
   "metadata": {},
   "source": [
    "## Single Step ALSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a645ac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=False)\n",
    "        self.lnorm = nn.LayerNorm(hidden_size)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: (B, T, I)\n",
    "        o, (h, _) = self.lstm(x) # o: (B, T, H) / h: (1, B, H)\n",
    "        normed_context = self.lnorm(h[-1, :, :])\n",
    "        return normed_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0efe08cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAttention(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=False)\n",
    "        self.lnorm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, rt_attn: bool=False):\n",
    "        # x: (B, T, I)\n",
    "        o, (h, _) = self.lstm(x) # o: (B, T, H) / h: (1, B, H)\n",
    "        h = h[-1, :, :]  # (B, H)\n",
    "        score = torch.bmm(o, h.unsqueeze(-1)) # (B, T, H) x (B, H, 1)\n",
    "        attn = torch.softmax(score, 1).squeeze(-1)  # (B, T)\n",
    "        context = torch.bmm(attn.unsqueeze(1), o).squeeze(1)  # (B, 1, T) x (B, T, H)\n",
    "        normed_context = self.lnorm(context)  # (B, H)\n",
    "        if rt_attn:\n",
    "            return normed_context, attn\n",
    "        else:\n",
    "            return normed_context, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5bf1200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PanelRegressionModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        feature_size: int, \n",
    "        embed_size: int,\n",
    "        output_size: int,\n",
    "        num_layers: int, \n",
    "        drop_rate: float, \n",
    "        device: str\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Network\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        self.lstm_encoder = LSTMAttention(input_size=feature_size, hidden_size=embed_size, num_layers=num_layers)\n",
    "        self.layer_norm = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(embed_size, output_size),\n",
    "            nn.ReLU(inplace=False)\n",
    "        )\n",
    "\n",
    "\n",
    "        # Loss\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        # Meta Mode Support / Query\n",
    "        self._mode_query(False)\n",
    "\n",
    "    # Recoder\n",
    "    # self.recorder = MetricRecorder().to(device)\n",
    "\n",
    "    def meta_train(self):\n",
    "        self.train()\n",
    "\n",
    "    def meta_eval(self):\n",
    "        self.manual_model_eval()\n",
    "\n",
    "    def _mode_query(self, mode: bool=True):\n",
    "        # check if is support of query\n",
    "        self.is_query = mode\n",
    "\n",
    "    def manual_model_eval(self, mode: bool=False):\n",
    "        \"\"\"\n",
    "        [PyTorch Issue] RuntimeError: cudnn RNN backward can only be called in training mode\n",
    "        cannot use `model.eval()`. \n",
    "        see https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch\n",
    "        \"\"\"\n",
    "        for module in self.children():\n",
    "            self.training = mode\n",
    "            if isinstance(module, nn.Dropout) or isinstance(module, nn.LayerNorm):\n",
    "                module.train(mode)\n",
    "\n",
    "    def encode_lstm(self, inputs: torch.Tensor, rt_attn: bool=False):\n",
    "        \"\"\"forward data by each stock to avoid trained by other stocks\n",
    "        - B: number of samples (n_support if meta-learning)\n",
    "        - T: window size\n",
    "        - I: input size\n",
    "        - E: embedding size\n",
    "        - M: M = N * K\n",
    "\n",
    "        Args:\n",
    "            inputs: (B, T, I).\n",
    "            - support: (B, T, I) B: n_support\n",
    "            - query: (B, T, I) B: n_query\n",
    "\n",
    "        Returns:\n",
    "            encoded: (B, E)\n",
    "            attn: (B, T)\n",
    "        \"\"\"\n",
    "        B, T, I = inputs.size() # B = n_support\n",
    "        inputs = self.dropout(inputs)\n",
    "        encoded, attn = self.lstm_encoder(inputs, rt_attn)  # encoded: (B, E), attn: (B, T)\n",
    "        encoded = self.layer_norm(encoded)\n",
    "        return encoded, attn\n",
    "\n",
    "    def forward_encoder(self, inputs: torch.Tensor, rt_attn: bool=True):\n",
    "        \"\"\"Forward Encoder: from `inputs` to `z`\n",
    "        - B: number of n_support\n",
    "        - T: window size\n",
    "        - E: embedding size\n",
    "        - H: hidden size\n",
    "\n",
    "        Returns:\n",
    "            l: (B, O) # O: output feature dim\n",
    "            attn: (B, T). attention weights for each inputs.\n",
    "\n",
    "        \"\"\"\n",
    "        # support l: (B, T, E), attn: (B, T)\n",
    "        # query l: (B, T, E), attn: (B, T)\n",
    "\n",
    "        l, attn = self.encode_lstm(inputs, rt_attn=rt_attn)\n",
    "        e = self.encoder(l)  # e: (B, N, K, 2)\n",
    "\n",
    "        return e, attn\n",
    "\n",
    "    def forward(\n",
    "            self, data , # data = torch.tensor\n",
    "            rt_attn: bool=False\n",
    "        ):\n",
    "\n",
    "        e , attn = self.forward_encoder(data) # e: (B, O)\n",
    "        e = e.squeeze(dim=-1)\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3916d0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_size: 11 \n",
    "embed_size: 32\n",
    "output_size: 1\n",
    "num_layers: 1 \n",
    "drop_rate: 0.1\n",
    "device: 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6edff31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PanelRegressionModel(feature_size=11, embed_size=32, output_size=1, num_layers=1, drop_rate=0, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95481b68",
   "metadata": {},
   "source": [
    "## MAML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "19a72614",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maml_Regression_Trainer(nn.Module):\n",
    "    def __init__(\n",
    "        self, exp_name, log_dir, task_type,  model, batch_size,\n",
    "        n_inner_step, total_steps, \n",
    "        n_valid_step, every_valid_step, print_step,\n",
    "        inner_lr, outer_lr, device, clip_value, test_window_size):\n",
    "        \n",
    "        super(Maml_Regression_Trainer, self).__init__()\n",
    "        self.exp_name = exp_name\n",
    "        self.log_dir = Path(log_dir).resolve()\n",
    "        self.device = device\n",
    "        self.model = model.to(self.device)\n",
    "        if task_type == \"classification\":\n",
    "            self.loss_fn = nn.NLLLoss() \n",
    "        elif task_type == \"regression\":\n",
    "            self.loss_fn = nn.MSELoss()\n",
    "            \n",
    "        self.n_inner_step = n_inner_step\n",
    "        self.total_steps = total_steps\n",
    "        self.n_valid_step = n_valid_step\n",
    "        self.every_valid_step = every_valid_step\n",
    "        self.print_step = print_step\n",
    "        self.inner_lr = inner_lr\n",
    "        self.outer_lr = outer_lr\n",
    "        self.batch_size = batch_size\n",
    "        self.test_window_size = test_window_size\n",
    "        self.keep_weights = self.clone_weight(self.model)\n",
    "        self.meta_optim = torch.optim.Adam(\n",
    "            self.model.parameters(), \n",
    "            lr=self.outer_lr\n",
    "        )\n",
    "        if self.device == 'cuda':\n",
    "            self.cuda()\n",
    "        \n",
    "        # Recoder\n",
    "        self.train_recorder = RegressionMetricRecorder().to(device)\n",
    "        self.valid_recorder = RegressionMetricRecorder().to(device)\n",
    "        self.test_recorder = RegressionMetricRecorder().to(device)\n",
    "        \n",
    "        self.task_recorder = RegressionMetricTaskRecorder().to(device)\n",
    " \n",
    "\n",
    "        \n",
    "    def init_experiments(self, exp_num=None, record_tensorboard: bool=True):\n",
    "        # check if exp exists\n",
    "        exp_dirs = sorted(list(self.log_dir.glob(f'{self.exp_name}_*')))\n",
    "        if exp_num is None:\n",
    "            exp_num = int(exp_dirs[-1].name[len(self.exp_name)+1:]) if exp_dirs else 0\n",
    "            self.exp_num = exp_num + 1\n",
    "        else:\n",
    "            self.exp_num = exp_num\n",
    "        self.exp_dir = self.log_dir / f'{self.exp_name}_{self.exp_num}'\n",
    "        if record_tensorboard:\n",
    "            self.writer = SummaryWriter(str(self.exp_dir))\n",
    "        else:\n",
    "            self.writer = None\n",
    "        self.ckpt_path = self.exp_dir / 'checkpoints'\n",
    "        self.ckpt_step_train_path =  self.ckpt_path / 'step' / 'train'\n",
    "        self.ckpt_step_valid_path =  self.ckpt_path / 'step' / 'valid'\n",
    "        for p in [self.ckpt_path, self.ckpt_step_train_path, self.ckpt_step_valid_path]:\n",
    "            if not p.exists():\n",
    "                p.mkdir(parents=True)    \n",
    "    \n",
    "    def get_acc(self,y_true, y_pred):\n",
    "        correct = torch.eq(y_pred, y_true).sum().item()\n",
    "        acc = correct/ len(y_true)\n",
    "        return acc\n",
    "\n",
    "    def clone_weight(self, model):\n",
    "        return {k: v.clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    def meta_update(self, dummy_loss, sum_grads):\n",
    "        # Update theta_parameter by sum_gradients\n",
    "        hooks = []\n",
    "        for k,v in enumerate(self.model.parameters()):\n",
    "            def closure():\n",
    "                key = k\n",
    "                return lambda grad: sum_grads[key]\n",
    "            hooks.append(v.register_hook(closure()))\n",
    "\n",
    "        self.meta_optim.zero_grad()\n",
    "        dummy_loss.backward()\n",
    "        self.meta_optim.step()\n",
    "\n",
    "        for h in hooks:\n",
    "            h.remove()\n",
    "                \n",
    "    # inner loop per 1 task\n",
    "    def inner_loop(self, support_x, support_y, query_x, query_y, is_meta_train):\n",
    "        updated_state_dict = {k: v.clone() for k, v in self.keep_weights.items()}\n",
    "        for i in range(self.n_inner_step):\n",
    "            if i > 0:\n",
    "                self.model.load_state_dict(updated_state_dict)\n",
    "            support_e = self.model(support_x)\n",
    "            s_loss = self.loss_fn(support_e, support_y)\n",
    "            grad = torch.autograd.grad(\n",
    "                    s_loss, \n",
    "                    self.model.parameters(),\n",
    "                    create_graph=True,\n",
    "                )\n",
    "            for i, (k,w) in enumerate(updated_state_dict.items()):\n",
    "                updated_state_dict[k] = updated_state_dict[k] - self.inner_lr * grad[i].data\n",
    "        \n",
    "        s_mse = self.task_recorder.metrics['Support_MSE'](support_e, support_y)\n",
    "        s_mae = self.task_recorder.metrics['Support_MAE'](support_e, support_y)\n",
    "        s_mape = self.task_recorder.metrics['Support_MAPE'](support_e, support_y)\n",
    "       \n",
    "        self.model.load_state_dict(updated_state_dict)\n",
    "        query_e = self.model(query_x)\n",
    "        q_loss = self.loss_fn(query_e, query_y)\n",
    "        \n",
    "        q_mse = self.task_recorder.metrics['Query_MSE'](query_e, query_y)\n",
    "        q_mae = self.task_recorder.metrics['Query_MAE'](query_e, query_y)\n",
    "        q_mape = self.task_recorder.metrics['Query_MAPE'](query_e, query_y)\n",
    "        \n",
    "        \n",
    "        if is_meta_train:\n",
    "            q_grad = torch.autograd.grad(q_loss, self.model.parameters(), create_graph=True)\n",
    "        else:\n",
    "            q_grad = None\n",
    "        \n",
    "        \n",
    "        return s_mse, s_mae, s_mape, q_mse, q_mae, q_mape, q_grad, query_e\n",
    "\n",
    "    # outer loop per batch\n",
    "    def outer_loop(self, meta_dataset):\n",
    "        self.model.meta_train()\n",
    "        batch_task, window_size = meta_dataset.generate_batch_task(all_tasks=meta_dataset.all_tasks) # PanelDataDict\n",
    "        train_tasks  = PanelDataDict(batch_task,window_size = window_size)\n",
    "        train_tasks.to(self.device)\n",
    "        all_q_grads = []\n",
    "\n",
    "        self.keep_weights = self.clone_weight(self.model)\n",
    "        \n",
    "        for i  in range(self.batch_size):\n",
    "            x_spt = train_tasks['support'][i]\n",
    "            y_spt = train_tasks['support_labels'][i]\n",
    "            x_qry = train_tasks['query'][i]\n",
    "            y_qry = train_tasks['query_labels'][i]\n",
    "            s_mse, s_mae, s_mape, q_mse, q_mae, q_mape, q_grad, query_e = self.inner_loop(x_spt, y_spt, x_qry, y_qry, is_meta_train=True)\n",
    "            self.train_recorder.update('Support_MSE', s_mse)\n",
    "            self.train_recorder.update('Support_MAE', s_mae)\n",
    "            self.train_recorder.update('Support_MAPE', s_mape)\n",
    "            self.train_recorder.update('Query_MSE', q_mse)\n",
    "            self.train_recorder.update('Query_MAE', q_mae)\n",
    "            self.train_recorder.update('Query_MAPE', q_mape)\n",
    "            \n",
    "            \n",
    "            all_q_grads.append(q_grad)\n",
    "            self.model.load_state_dict(self.keep_weights)\n",
    "            \n",
    "        \n",
    "        sum_q_grads = [torch.stack(grads).sum(dim=0) for grads in list(zip(*all_q_grads))]\n",
    "        \n",
    "        x_spt = train_tasks['support'][0]\n",
    "        y_spt = train_tasks['support_labels'][0]\n",
    "        \n",
    "        dummy_e = self.model(x_spt)\n",
    "        dummy_loss = self.loss_fn(dummy_e, y_spt)\n",
    "        \n",
    "        self.meta_update(dummy_loss, sum_q_grads)\n",
    "        return \n",
    "\n",
    "\n",
    "    def meta_train(self, meta_trainset,\n",
    "                meta_validset_time,\n",
    "                meta_validset_entity,\n",
    "                meta_validset_mix, \n",
    "                print_log: bool=True):\n",
    "        \n",
    "        best_eval_mse = 10000.0\n",
    "        for step in range(self.total_steps):\n",
    "            self.train_recorder.reset()\n",
    "            # Meta-Train per epoch\n",
    "            self.outer_loop(meta_trainset)\n",
    "            if ( step % self.print_step == 0) or (step == self.total_steps-1):\n",
    "                prefix = 'Train'\n",
    "                train_logs = self.train_recorder.compute(prefix)\n",
    "                cur_eval_mse = train_logs[f'{prefix}-Query_MSE']\n",
    "                cur_eval_mae = train_logs[f'{prefix}-Query_MAE']\n",
    "                cur_eval_mape = train_logs[f'{prefix}-Query_MAPE']\n",
    "                \n",
    "                self.log_results(train_logs, prefix, step=step, total_steps=self.total_steps, print_log=True)\n",
    "                torch.save(self.model.state_dict(), str(self.ckpt_step_train_path / f'{step}-{cur_eval_mse:.4f}.ckpt'))\n",
    "\n",
    "                \n",
    "            # Meta-Valid\n",
    "            if (self.every_valid_step != 0):\n",
    "                if (step % self.every_valid_step == 0) or (step == self.total_steps-1):\n",
    "                    ref_step = step\n",
    "                    \n",
    "                    prefix = 'Valid-time'\n",
    "                    valid_time_logs, cur_eval_mse_time, cur_eval_mae_time, cur_eval_mape_time = self.meta_valid(self.model, meta_validset_time, prefix, ref_step, self.n_valid_step)\n",
    "                    \n",
    "                    prefix = 'Valid-entity'\n",
    "                    valid_entity_logs, cur_eval_mse_entity, cur_eval_mae_entity, cur_eval_mape_entity = self.meta_valid(self.model, meta_validset_entity, prefix, ref_step, self.n_valid_step)\n",
    "                    \n",
    "                    prefix = 'Valid-mix'\n",
    "                    valid_mix_logs, cur_eval_mse_mix, cur_eval_mae_mix, cur_eval_mape_mix = self.meta_valid(self.model, meta_validset_mix, prefix, ref_step, self.n_valid_step)\n",
    "                    \n",
    "                    prefix = 'Valid'\n",
    "                    cur_eval_mse = (cur_eval_mse_time + cur_eval_mse_entity + cur_eval_mse_mix) / 3\n",
    "                    cur_eval_mae = (cur_eval_mae_time + cur_eval_mae_entity + cur_eval_mae_mix) / 3\n",
    "                    cur_eval_mape = (cur_eval_mape_time + cur_eval_mape_entity + cur_eval_mape_mix) / 3\n",
    "                    valid_final_log = {f'{prefix}-AvgMSE': cur_eval_mse, f'{prefix}-AvgMAE': cur_eval_mae, f'{prefix}-AvgMAPE': cur_eval_mape}\n",
    "                    self.log_results(valid_final_log, prefix, step=ref_step, total_steps=self.total_steps, print_log=print_log)\n",
    "                    \n",
    "                    # save best\n",
    "                    if (cur_eval_mse < best_eval_mse):\n",
    "                        best_eval_mse = cur_eval_mse \n",
    "                        torch.save(self.model.state_dict(), str(self.ckpt_step_valid_path / f'{ref_step:06d}-{cur_eval_mse:.4f}.ckpt'))\n",
    "                    \n",
    "    def meta_valid(self, model, meta_dataset, prefix, ref_step, n_valid, print_log=True):\n",
    "        self.valid_recorder.reset()\n",
    "        valid_logs = self.run_valid(model, meta_dataset, n_valid, prefix)\n",
    "        self.log_results(valid_logs, prefix, step=ref_step, total_steps=self.total_steps, print_log=print_log)\n",
    "        cur_eval_mse = valid_logs[f'{prefix}-Query_MSE']\n",
    "        cur_eval_mae = valid_logs[f'{prefix}-Query_MAE']\n",
    "        cur_eval_mape = valid_logs[f'{prefix}-Query_MAPE']\n",
    "        return valid_logs, cur_eval_mse, cur_eval_mae, cur_eval_mape\n",
    "        \n",
    "    def meta_test(self, model, meta_dataset,  print_log: bool=True):\n",
    "        self.test_recorder.reset()\n",
    "        prefix = meta_dataset.meta_type.capitalize()\n",
    "        test_logs = self.run_test(model, meta_dataset, prefix)\n",
    "        self.log_results(test_logs, prefix, step=0, total_steps=0, print_log=print_log)\n",
    "        eval_mse = test_logs[f'{prefix}-Query_MSE']\n",
    "        eval_mae = test_logs[f'{prefix}-Query_MAE']\n",
    "        eval_mape = test_logs[f'{prefix}-Query_MAPE']\n",
    "        return prefix, eval_mse, eval_mae, eval_mape\n",
    "    \n",
    "    def run_valid(self, model, meta_dataset, n_valid, prefix):\n",
    "        model = model.to(self.device)\n",
    "        model.meta_eval()\n",
    "        pregress = tqdm(range(n_valid), total= n_valid, desc=f'Running {prefix}')\n",
    "     \n",
    "        for val_idx in pregress:\n",
    "            batch_task, window_size = meta_dataset.generate_batch_task(all_tasks=meta_dataset.all_tasks) # PanelDataDict\n",
    "            valid_tasks  = PanelDataDict(batch_task,window_size = window_size)\n",
    "            valid_tasks.to(self.device)\n",
    "            for i  in range(self.batch_size):\n",
    "                x_spt = valid_tasks['support'][i]\n",
    "                y_spt = valid_tasks['support_labels'][i]\n",
    "                x_qry = valid_tasks['query'][i]\n",
    "                y_qry = valid_tasks['query_labels'][i]\n",
    "                s_mse, s_mae, s_mape, q_mse, q_mae, q_mape, q_grad, query_e = self.inner_loop(x_spt, y_spt, x_qry, y_qry, is_meta_train=False)\n",
    "                self.valid_recorder.update('Support_MSE', s_mse)\n",
    "                self.valid_recorder.update('Support_MAE', s_mae)\n",
    "                self.valid_recorder.update('Support_MAPE', s_mape)\n",
    "                self.valid_recorder.update('Query_MSE', q_mse)\n",
    "                self.valid_recorder.update('Query_MAE', q_mae)\n",
    "                self.valid_recorder.update('Query_MAPE', q_mape)\n",
    "        \n",
    "        \n",
    "        pregress.close()\n",
    "        valid_logs = self.valid_recorder.compute(prefix)       \n",
    "        return valid_logs\n",
    "    \n",
    "    def run_test(self, model, meta_dataset, prefix):\n",
    "        model = model.to(self.device)\n",
    "        model.meta_eval()\n",
    "        test_all_tasks = meta_dataset.all_tasks[self.test_window_size]\n",
    "        test_tasks = PanelDataDict(test_all_tasks, window_size = self.test_window_size)\n",
    "        test_tasks.to(self.device)\n",
    "        pregress = tqdm(range(len(test_tasks['query'])), total= len(test_tasks['query']), desc=f'Running {prefix}')\n",
    "        for test_idx in pregress:\n",
    "            x_spt = test_tasks['support'][test_idx]\n",
    "            y_spt = test_tasks['support_labels'][test_idx]\n",
    "            x_qry = test_tasks['query'][test_idx]\n",
    "            y_qry = test_tasks['query_labels'][test_idx]\n",
    "            s_mse, s_mae, s_mape, q_mse, q_mae, q_mape, q_grad, query_e = self.inner_loop(x_spt, y_spt, x_qry, y_qry, is_meta_train=False)\n",
    "            self.test_recorder.update('Support_MSE', s_mse)\n",
    "            self.test_recorder.update('Support_MAE', s_mae)\n",
    "            self.test_recorder.update('Support_MAPE', s_mape)\n",
    "            self.test_recorder.update('Query_MSE', q_mse)\n",
    "            self.test_recorder.update('Query_MAE', q_mae)\n",
    "            self.test_recorder.update('Query_MAPE', q_mape)\n",
    "        \n",
    "        pregress.close()\n",
    "        test_logs = self.test_recorder.compute(prefix)       \n",
    "        return test_logs\n",
    "    \n",
    "    def log_results(self, logs, prefix, step, total_steps, print_log=False):\n",
    "        \n",
    "        for log_string, value in logs.items():       \n",
    "            if self.writer is not None:\n",
    "                self.writer.add_scalar(log_string, value, step)\n",
    "                \n",
    "        def extract(prefix, key, logs):\n",
    "            mean = logs[f'{prefix}-{key}']\n",
    "            s = ''\n",
    "            s += f'{mean:.4f}'\n",
    "            return s\n",
    "\n",
    "        if print_log:\n",
    "            only_one_to_print = True if prefix in ['Valid', 'Test'] else False\n",
    "\n",
    "            if only_one_to_print:\n",
    "                avgmse = extract(prefix, 'AvgMSE', logs)\n",
    "                avgmae = extract(prefix, 'AvgMAE', logs)\n",
    "                avgmape = extract(prefix, 'AvgMAPE', logs)\n",
    "         \n",
    "                print(f'[Meta {prefix}] Result - AvgMSE: {avgmse}, AvgMAE: {avgmae}, AvgMAPE: {avgmape} ')\n",
    "                print()\n",
    "\n",
    "            else:\n",
    "                s_mse = extract(prefix, 'Support_MSE', logs)\n",
    "                s_mae = extract(prefix, 'Support_MAE', logs)\n",
    "                s_mape = extract(prefix, 'Support_MAPE', logs)\n",
    "                q_mse = extract(prefix, 'Query_MSE', logs)\n",
    "                q_mae = extract(prefix, 'Query_MAE', logs)\n",
    "                q_mape = extract(prefix, 'Query_MAPE', logs)\n",
    "\n",
    "                print(f'[Meta {prefix}]({step+1}/{total_steps})')\n",
    "                print(f'  - [Support] MSE: {s_mse}, MAE: {s_mae}, MAPE: {s_mape}')\n",
    "                print(f'  - [Query] MSE: {q_mse}, MAE: {q_mae}, MAPE: {q_mape}')\n",
    "                print()\n",
    "                \n",
    "    def get_best_results(self, exp_num, record_tensorboard: bool=True):\n",
    "        self.init_experiments(exp_num=exp_num, record_tensorboard=record_tensorboard)\n",
    "        best_ckpt = sorted(\n",
    "            (self.ckpt_step_valid_path).glob('*.ckpt'),\n",
    "            key=lambda x: x.name.split('-')[1], \n",
    "            reverse=True\n",
    "        )[0]\n",
    "        \n",
    "        best_step, train_loss = best_ckpt.name.rstrip('.ckpt').split('-')\n",
    "        state_dict = torch.load(best_ckpt)\n",
    "        return int(best_step), float(train_loss), state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e6a1e7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionMetricRecorder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        cs = tm.MetricCollection({\n",
    "            'Support_MSE': tm.MeanMetric(), \n",
    "            'Support_MAE': tm.MeanMetric(),\n",
    "            'Support_MAPE': tm.MeanMetric(),\n",
    "            'Query_MSE': tm.MeanMetric(), \n",
    "            'Query_MAE': tm.MeanMetric(),\n",
    "            'Query_MAPE': tm.MeanMetric(),\n",
    "        })\n",
    "\n",
    "        self.metrics = cs.clone()\n",
    "    @property\n",
    "    def keys(self):\n",
    "        return list(self.metrics.keys())\n",
    "    \n",
    "    def update(self, key, scores):\n",
    "        self.metrics[key].update(scores)\n",
    "            \n",
    "    def compute(self, prefix: str):\n",
    "        results = {}\n",
    "        for k in self.keys:\n",
    "            m = self.metrics[k].compute()\n",
    "            if isinstance(m, torch.Tensor):\n",
    "                m = m.cpu().detach().numpy()\n",
    "            results[f'{prefix}-{k}'] = m\n",
    "        return results\n",
    "    \n",
    "    def reset(self):\n",
    "        for k in self.keys:\n",
    "            self.metrics[k].reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "11aed9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionMetricTaskRecorder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        cs = tm.MetricCollection({\n",
    "            'Support_MSE': tm.MeanSquaredError(), \n",
    "            'Support_MAE': tm.MeanAbsoluteError(),\n",
    "            'Support_MAPE': tm.MeanAbsolutePercentageError(),\n",
    "            'Query_MSE': tm.MeanSquaredError(), \n",
    "            'Query_MAE': tm.MeanAbsoluteError(),\n",
    "            'Query_MAPE': tm.MeanAbsolutePercentageError(),\n",
    "        })\n",
    "        \n",
    "        self.metrics = cs.clone()\n",
    "        \n",
    "    @property\n",
    "    def keys(self):\n",
    "        return list(self.metrics.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a9b16166",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'kdd17_0'\n",
    "log_dir = './logging'\n",
    "task_type = \"regression\"\n",
    "model = PanelRegressionModel(feature_size=11, embed_size=32, output_size=1, num_layers=1, drop_rate=0, device='cuda')\n",
    "batch_size = 64\n",
    "n_inner_step = 2\n",
    "total_steps = 2\n",
    "n_valid_step = 2\n",
    "every_valid_step = 2\n",
    "print_step = 1\n",
    "inner_lr = 0.01\n",
    "outer_lr = 0.001\n",
    "device = 'cuda'\n",
    "clip_value = 0\n",
    "test_window_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d813647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "maml_train = Maml_Regression_Trainer(exp_name,log_dir, task_type, model, batch_size, n_inner_step, total_steps, n_valid_step, every_valid_step, print_step, inner_lr, outer_lr, device, clip_value, test_window_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "099a7049",
   "metadata": {},
   "outputs": [],
   "source": [
    "maml_train.init_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bda8ad06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta Train](1/2)\n",
      "  - [Support] MSE: 2.9967, MAE: 1.1789, MAPE: 3500.3667\n",
      "  - [Query] MSE: 2.2061, MAE: 1.0476, MAPE: 1.0913\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Valid-time: 100%|███████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta Valid-time](1/2)\n",
      "  - [Support] MSE: 2.1760, MAE: 1.0574, MAPE: 1795.5468\n",
      "  - [Query] MSE: 3.0426, MAE: 1.2335, MAPE: 1.2080\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Valid-entity: 100%|█████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta Valid-entity](1/2)\n",
      "  - [Support] MSE: 4.1395, MAE: 1.2132, MAPE: 1.0871\n",
      "  - [Query] MSE: 3.8890, MAE: 1.2704, MAPE: 1.0731\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Valid-mix: 100%|████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta Valid-mix](1/2)\n",
      "  - [Support] MSE: 3.1395, MAE: 1.1972, MAPE: 1.1013\n",
      "  - [Query] MSE: 3.6928, MAE: 1.2906, MAPE: 1.0104\n",
      "\n",
      "[Meta Valid] Result - AvgMSE: 3.5415, AvgMAE: 1.2648, AvgMAPE: 1.0972 \n",
      "\n",
      "[Meta Train](2/2)\n",
      "  - [Support] MSE: 5.5980, MAE: 1.3855, MAPE: 1.0214\n",
      "  - [Query] MSE: 5.4727, MAE: 1.3126, MAPE: 1.0275\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Valid-time: 100%|███████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta Valid-time](2/2)\n",
      "  - [Support] MSE: 2.1751, MAE: 1.0351, MAPE: 26.6098\n",
      "  - [Query] MSE: 1.8135, MAE: 1.0152, MAPE: 1.4320\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Valid-entity: 100%|█████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta Valid-entity](2/2)\n",
      "  - [Support] MSE: 3.1640, MAE: 1.2556, MAPE: 1.2167\n",
      "  - [Query] MSE: 5.8260, MAE: 1.4548, MAPE: 1.1026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Valid-mix: 100%|████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta Valid-mix](2/2)\n",
      "  - [Support] MSE: 2.9875, MAE: 1.1575, MAPE: 415.5214\n",
      "  - [Query] MSE: 2.2653, MAE: 1.1087, MAPE: 1.2722\n",
      "\n",
      "[Meta Valid] Result - AvgMSE: 3.3016, AvgMAE: 1.1929, AvgMAPE: 1.2689 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "maml_train.meta_train(meta_train, meta_valid_time, meta_valid_entity, meta_valid_mix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7bffa8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Test-time: 100%|█████████████████████████████████████████████████████████████| 8470/8470 [01:21<00:00, 103.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta Test-time](1/0)\n",
      "  - [Support] MSE: 2.4698, MAE: 1.0361, MAPE: 920.6209\n",
      "  - [Query] MSE: 2.3654, MAE: 1.0025, MAPE: 164.9027\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Test-time',\n",
       " array(2.3653708, dtype=float32),\n",
       " array(1.0024823, dtype=float32),\n",
       " array(164.90266, dtype=float32))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maml_train.meta_test(maml_train.model, meta_test_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3d04d1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_stock(p: Path | str):\n",
    "        def longterm_trend(x: pd.Series, k:int):\n",
    "            return (x.rolling(k).sum().div(k*x) - 1) * 100\n",
    "\n",
    "        df = pd.read_csv(p)\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df = df.sort_values('Date').reset_index(drop=True)\n",
    "        if 'Unnamed' in df.columns:\n",
    "            df.drop(columns=df.columns[7], inplace=True)\n",
    "        if 'Original_Open' in df.columns:\n",
    "            df.rename(columns={'Original_Open': 'Open', 'Open': 'Adj Open'}, inplace=True)\n",
    "\n",
    "        # Open, High, Low\n",
    "        z1 = (df.loc[:, ['Open', 'High', 'Low']].div(df['Close'], axis=0) - 1).rename(\n",
    "            columns={'Open': 'open', 'High': 'high', 'Low': 'low'}) * 100\n",
    "        # Close\n",
    "        z2 = df[['Close']].pct_change().rename(columns={'Close': 'close'}) * 100\n",
    "        # Adj Close\n",
    "        z3 = df[['Adj Close']].pct_change().rename(columns={'Adj Close': 'adj_close'}) * 100\n",
    "\n",
    "        z4 = []\n",
    "        for k in [5, 10, 15, 20, 25, 30]:\n",
    "            z4.append(df[['Adj Close']].apply(longterm_trend, k=k).rename(columns={'Adj Close': f'zd{k}'}))\n",
    "\n",
    "        df_pct = pd.concat([df['Date'], z1, z2, z3] + z4, axis=1).rename(columns={'Date': 'date'})\n",
    "        cols_max = df_pct.columns[df_pct.isnull().sum() == df_pct.isnull().sum().max()]\n",
    "        df_pct = df_pct.loc[~df_pct[cols_max].isnull().values, :]\n",
    "\n",
    "        return df_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecdc16e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/david/yjhwang/TEAP/data/kdd17/price_long_50')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_config['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d431a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = list((ds_config['path']).glob('*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5626e927",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ceafa564",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_2 = ps[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eaa32091",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_symbol = p.name.rstrip('.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2e4a5802",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_symbol_2 = p_2.name.rstrip('.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "24fe48a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = [p,p_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d245e60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = load_single_stock(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b001eb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = load_single_stock(p_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f0cf7223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2489, 12)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f5ef2529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2489, 12)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d1f90ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_idx(df_single, window_size, n_support, n_query):\n",
    "    if len(df_single) >= window_size:\n",
    "        x_spt_task = []\n",
    "        y_spt_task = []\n",
    "        x_qry_task = []\n",
    "        y_qry_task = []\n",
    "\n",
    "        for i in range(len_df-window_size-n_support-n_query+1):\n",
    "            x_spt = []\n",
    "            y_spt = []\n",
    "            x_qry = []\n",
    "            y_qry = []\n",
    "\n",
    "            for j in range(n_support+n_query):\n",
    "                if j < n_support:\n",
    "                    spt_idx = [idx for idx in range(i+j, i+j+window_size)]\n",
    "                    x_spt.append(spt_idx)\n",
    "                    y_spt.append(i+j+window_size)\n",
    "\n",
    "                else:\n",
    "                    qry_idx = [idx for idx in range(i+j, i+j+window_size)]\n",
    "                    x_qry.append(qry_idx)\n",
    "                    y_qry.append(i+j+window_size)\n",
    "\n",
    "            x_spt_task.append(x_spt)\n",
    "            y_spt_task.append(y_spt)\n",
    "            x_qry_task.append(x_qry)\n",
    "            y_qry_task.append(y_qry)\n",
    "        return x_spt_task, y_spt_task, x_qry_task, y_qry_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e6c47422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(df_single, x_spt_task, y_spt_task, x_qry_task, y_qry_task, n_support, n_query):\n",
    "    num_task = len(x_spt_task)\n",
    "    support_task = []\n",
    "    support_labels = []\n",
    "    query_task = []\n",
    "    query_labels = []\n",
    "    for i in range(len(x_spt_task)):\n",
    "        support_inputs = []\n",
    "        query_inputs = []\n",
    "        for j in range(n_support):\n",
    "            support_inputs.append(df_single.iloc[x_spt_task[i][j]].to_numpy()[:, 1:].astype(np.float64))\n",
    "        \n",
    "        support_labels.append(df_single['close'].iloc[y_spt_task[i]].to_numpy().astype(np.float64))\n",
    "        support_task.append(np.array(support_inputs))\n",
    "        for k in range(n_query):\n",
    "            query_inputs.append(df_single.iloc[x_qry_task[i][k]].to_numpy()[:, 1:].astype(np.float64))\n",
    "        query_labels.append(df_single['close'].iloc[y_qry_task[i]].to_numpy().astype(np.float64))\n",
    "        query_task.append(np.array(query_inputs))   \n",
    "        \n",
    "    return support_task, support_labels, query_task, query_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "70b69cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size, n_support, n_query = 3, 5, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "8eb04de3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_spt_task, y_spt_task, x_qry_task, y_qry_task = sliding_window_idx(df_2, window_size, n_support, n_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ea9e6084",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_inputs, support_labels, query_inputs, query_labels = generate_data(df_2, x_spt_task, y_spt_task, x_qry_task, y_qry_task, n_support, n_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "aa902814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support input: 12\n",
      "support label: 12\n",
      "query input: 12\n",
      "query label: 12\n"
     ]
    }
   ],
   "source": [
    "print(f'support input: {len(support_inputs)}') # (n_task, n_support, widow_size, feature_dim)\n",
    "print(f'support label: {len(support_labels)}') # (n_task, n_support)\n",
    "print(f'query input: {len(query_inputs)}')\n",
    "print(f'query label: {len(query_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "46968795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3, 11)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "support_inputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a154ab0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "support_labels[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "fcd51cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.26134388,  3.84167883,  0.34753364, -0.49156072, -0.62871896])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "support_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4ac2eacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tasks = dict(\n",
    "            query = [],\n",
    "            query_labels = [],\n",
    "            support = [],\n",
    "            support_labels = [],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a2222285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_task(symbol):\n",
    "    all_tasks = dict(\n",
    "            query = [],\n",
    "            query_labels = [],\n",
    "            support = [],\n",
    "            support_labels = [],\n",
    "        )\n",
    "    for s in symbol:\n",
    "        df = load_single_stock(s)\n",
    "        x_spt_task, y_spt_task, x_qry_task, y_qry_task = sliding_window_idx(df, window_size, n_support, n_query)\n",
    "        support_inputs, support_labels, query_inputs, query_labels = generate_data(df, x_spt_task, y_spt_task, x_qry_task, y_qry_task, n_support, n_query)\n",
    "        all_tasks['query'].extend(query_inputs)\n",
    "        all_tasks['query_labels'].extend(query_labels)\n",
    "        all_tasks['support'].extend(support_inputs)\n",
    "        all_tasks['support_labels'].extend(support_labels)\n",
    "        num_task = len(all_tasks['query'])\n",
    "    return num_task, all_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "bd4f8994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_task(batch_size, all_tasks, num_task):\n",
    "    batch_tasks = dict(\n",
    "            query = [],\n",
    "            query_labels = [],\n",
    "            support = [],\n",
    "            support_labels = [],\n",
    "        )\n",
    "    for k, v in all_tasks.items():\n",
    "            all_tasks[k] = np.array(v)\n",
    "            \n",
    "    batch_idx = random.sample(list(range(num_task)), batch_size)\n",
    "    batch_tasks['query'] = all_tasks['query'][batch_idx]\n",
    "    batch_tasks['query_labels'] = all_tasks['query_labels'][batch_idx]\n",
    "    batch_tasks['support'] = all_tasks['support'][batch_idx]\n",
    "    batch_tasks['support_labels'] = all_tasks['support_labels'][batch_idx]\n",
    "    \n",
    "    return batch_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "e30dc449",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_task, all_tasks = generate_all_task(symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "7d534598",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tasks = generate_batch_task(batch_size=2, all_tasks=all_tasks, num_task=num_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "bb8133cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockRegressionDataDict(dict):\n",
    "    def __init__(self, data, window_size):\n",
    "        self.window_size = window_size\n",
    "        self._set_state(f'numpy')\n",
    "        for k, v in data.items():\n",
    "            data[k] = np.array(v)\n",
    "        \n",
    "        self.n_stocks = len(v)\n",
    "        super().__init__(data)\n",
    "    \n",
    "    def tensor_fn(self, value, key):\n",
    "        return torch.FloatTensor(value)\n",
    "\n",
    "    def _set_state(self, state: str):\n",
    "        self.state = state\n",
    "\n",
    "    def to(self, device: None | str=None):\n",
    "        if device is None:\n",
    "            device = torch.device('cpu')\n",
    "        else:\n",
    "            device = torch.device(device)\n",
    "        self._set_state(f'tensor.{device}')\n",
    "        for key in self.keys():\n",
    "            value = self.__getitem__(key)\n",
    "            tvalue = self.tensor_fn(value, key)\n",
    "            self.__setitem__(key, tvalue.to(device)) \n",
    "        \n",
    "    def numpy(self):\n",
    "        self._set_state('numpy')\n",
    "        for key in self.keys():\n",
    "            tvalue = self.__getitem__(key)\n",
    "            if not isinstance(tvalue, np.ndarray): \n",
    "                self.__setitem__(key, tvalue.detach().numpy())\n",
    "\n",
    "    def __str__(self):\n",
    "        s = f'StockDataDict(T={self.window_size}, {self.state})\\n'\n",
    "        for i, key in enumerate(self.keys()):\n",
    "            value = self.__getitem__(key)\n",
    "            s += f'- {key}: {value.shape}'\n",
    "            s += '' if i == len(self.keys())-1 else '\\n'\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "67f44378",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = StockRegressionDataDict(batch_tasks, window_size = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "9faadad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "9ce3e2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': tensor([[[[-0.5876,  1.1242, -2.7082,  0.7983,  0.7983,  2.5192,  3.2013,\n",
       "             2.0082,  0.6745, -0.4854, -1.2936],\n",
       "           [ 1.2098,  1.2098, -2.0592, -0.7409, -0.7409,  2.1776,  3.6396,\n",
       "             2.7473,  1.5779,  0.4211, -0.4736],\n",
       "           [ 1.6715,  3.1308,  0.0000, -2.9858, -2.9858,  3.6827,  6.2006,\n",
       "             5.6602,  4.5702,  3.5765,  2.6523]]],\n",
       " \n",
       " \n",
       "         [[[-3.4804,  1.4358, -3.8020,  2.8956,  2.8956, -0.4870, -0.3182,\n",
       "            -1.0522, -1.4536, -1.4592, -1.1601],\n",
       "           [ 1.5923,  2.4939, -0.2342, -1.8952, -1.8953,  0.5784,  1.6310,\n",
       "             0.7993,  0.4894,  0.4051,  0.6065],\n",
       "           [-0.4981,  2.6993, -0.6487,  1.0655,  1.0655, -0.9893,  0.7322,\n",
       "            -0.0278, -0.4790, -0.6098, -0.5383]]]], device='cuda:0'),\n",
       " 'query_labels': tensor([[-1.6981],\n",
       "         [ 2.1664]], device='cuda:0'),\n",
       " 'support': tensor([[[[-0.1697,  0.1454, -0.8240, -0.6023, -0.6023, -1.4542, -3.4755,\n",
       "            -5.0816, -6.3112, -7.0887, -7.2895],\n",
       "           [ 0.9756,  2.4390, -0.2683, -0.6301, -0.6301, -0.4098, -2.3707,\n",
       "            -3.9382, -5.2610, -6.1951, -6.4398],\n",
       "           [ 0.5395,  1.0299, -0.0981, -0.5366, -0.5366,  0.4806, -1.4321,\n",
       "            -3.0799, -4.2962, -5.3163, -5.6384]],\n",
       " \n",
       "          [[ 0.9756,  2.4390, -0.2683, -0.6301, -0.6301, -0.4098, -2.3707,\n",
       "            -3.9382, -5.2610, -6.1951, -6.4398],\n",
       "           [ 0.5395,  1.0299, -0.0981, -0.5366, -0.5366,  0.4806, -1.4321,\n",
       "            -3.0799, -4.2962, -5.3163, -5.6384],\n",
       "           [-0.0489,  0.7828, -1.1742,  0.2452,  0.2452,  0.5039, -1.1448,\n",
       "            -2.7479, -4.0374, -5.1703, -5.5855]],\n",
       " \n",
       "          [[ 0.5395,  1.0299, -0.0981, -0.5366, -0.5366,  0.4806, -1.4321,\n",
       "            -3.0799, -4.2962, -5.3163, -5.6384],\n",
       "           [-0.0489,  0.7828, -1.1742,  0.2452,  0.2452,  0.5039, -1.1448,\n",
       "            -2.7479, -4.0374, -5.1703, -5.5855],\n",
       "           [ 3.5024,  4.4038, -0.1288, -5.0147, -5.0147,  4.4296,  4.0690,\n",
       "             2.6732,  1.2091,  0.0299, -0.5468]],\n",
       " \n",
       "          [[-0.0489,  0.7828, -1.1742,  0.2452,  0.2452,  0.5039, -1.1448,\n",
       "            -2.7479, -4.0374, -5.1703, -5.5855],\n",
       "           [ 3.5024,  4.4038, -0.1288, -5.0147, -5.0147,  4.4296,  4.0690,\n",
       "             2.6732,  1.2091,  0.0299, -0.5468],\n",
       "           [-0.5876,  1.1242, -2.7082,  0.7983,  0.7983,  2.5192,  3.2013,\n",
       "             2.0082,  0.6745, -0.4854, -1.2936]],\n",
       " \n",
       "          [[ 3.5024,  4.4038, -0.1288, -5.0147, -5.0147,  4.4296,  4.0690,\n",
       "             2.6732,  1.2091,  0.0299, -0.5468],\n",
       "           [-0.5876,  1.1242, -2.7082,  0.7983,  0.7983,  2.5192,  3.2013,\n",
       "             2.0082,  0.6745, -0.4854, -1.2936],\n",
       "           [ 1.2098,  1.2098, -2.0592, -0.7409, -0.7409,  2.1776,  3.6396,\n",
       "             2.7473,  1.5779,  0.4211, -0.4736]]],\n",
       " \n",
       " \n",
       "         [[[ 1.4412,  1.4524, -1.0949,  0.3475,  0.3475, -2.8824, -4.0353,\n",
       "            -4.4643, -4.3928, -3.9964, -2.1417],\n",
       "           [ 0.1010,  1.4258, -0.2470, -0.4916, -0.4916, -1.5359, -3.2368,\n",
       "            -3.6683, -3.7622, -3.5222, -1.9550],\n",
       "           [ 1.5027,  1.6834, -1.0168, -0.6287, -0.6287, -0.0814, -2.0325,\n",
       "            -2.7756, -2.9765, -2.9113, -1.6092]],\n",
       " \n",
       "          [[ 0.1010,  1.4258, -0.2470, -0.4916, -0.4916, -1.5359, -3.2368,\n",
       "            -3.6683, -3.7622, -3.5222, -1.9550],\n",
       "           [ 1.5027,  1.6834, -1.0168, -0.6287, -0.6287, -0.0814, -2.0325,\n",
       "            -2.7756, -2.9765, -2.9113, -1.6092],\n",
       "           [ 2.8238,  3.7531, -0.6196, -5.1746, -5.1746,  4.9017,  3.2003,\n",
       "             2.5291,  2.1983,  2.2504,  3.3353]],\n",
       " \n",
       "          [[ 1.5027,  1.6834, -1.0168, -0.6287, -0.6287, -0.0814, -2.0325,\n",
       "            -2.7756, -2.9765, -2.9113, -1.6092],\n",
       "           [ 2.8238,  3.7531, -0.6196, -5.1746, -5.1746,  4.9017,  3.2003,\n",
       "             2.5291,  2.1983,  2.2504,  3.3353],\n",
       "           [-1.9029,  1.1701, -1.9029,  0.8102,  0.8102,  2.9736,  2.3602,\n",
       "             1.7413,  1.3214,  1.3771,  2.0128]],\n",
       " \n",
       "          [[ 2.8238,  3.7531, -0.6196, -5.1746, -5.1746,  4.9017,  3.2003,\n",
       "             2.5291,  2.1983,  2.2504,  3.3353],\n",
       "           [-1.9029,  1.1701, -1.9029,  0.8102,  0.8102,  2.9736,  2.3602,\n",
       "             1.7413,  1.3214,  1.3771,  2.0128],\n",
       "           [-3.4804,  1.4358, -3.8020,  2.8956,  2.8956, -0.4870, -0.3182,\n",
       "            -1.0522, -1.4536, -1.4592, -1.1601]],\n",
       " \n",
       "          [[-1.9029,  1.1701, -1.9029,  0.8102,  0.8102,  2.9736,  2.3602,\n",
       "             1.7413,  1.3214,  1.3771,  2.0128],\n",
       "           [-3.4804,  1.4358, -3.8020,  2.8956,  2.8956, -0.4870, -0.3182,\n",
       "            -1.0522, -1.4536, -1.4592, -1.1601],\n",
       "           [ 1.5923,  2.4939, -0.2342, -1.8952, -1.8953,  0.5784,  1.6310,\n",
       "             0.7993,  0.4894,  0.4051,  0.6065]]]], device='cuda:0'),\n",
       " 'support_labels': tensor([[ 0.2452, -5.0147,  0.7983, -0.7409, -2.9858],\n",
       "         [-5.1746,  0.8102,  2.8956, -1.8952,  1.0655]], device='cuda:0')}"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "a29d75b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StockDataDict(T=3, tensor.cuda)\n",
      "- query: torch.Size([2, 1, 3, 11])\n",
      "- query_labels: torch.Size([2, 1])\n",
      "- support: torch.Size([2, 5, 3, 11])\n",
      "- support_labels: torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "d5b112e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 3, 11)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_tasks['query'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "43e94c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_tasks['query_labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "d968b6f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(batch_tasks['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "2f1235b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(batch_tasks['query_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "d16d1777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tasks['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f22e7262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5, 3, 11)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tasks['support'][[1,2,3]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "c48230de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(all_tasks['support']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "efb1738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_fn(value, key):\n",
    "    return torch.FloatTensor(value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "77c7fb1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 5, 3, 11])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor(all_tasks['support']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "3e95b730",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "for key,value in batch_tasks.items():\n",
    "            tvalue = tensor_fn(value, key)\n",
    "            batch_tasks[key] = tvalue.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "35bb09d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': tensor([[[[ 1.2098,  1.2098, -2.0592, -0.7409, -0.7409,  2.1776,  3.6396,\n",
       "             2.7473,  1.5779,  0.4211, -0.4736],\n",
       "           [ 1.6715,  3.1308,  0.0000, -2.9858, -2.9858,  3.6827,  6.2006,\n",
       "             5.6602,  4.5702,  3.5765,  2.6523],\n",
       "           [ 0.2699,  3.4278, -0.0270, -1.6981, -1.6981,  3.4062,  7.1498,\n",
       "             7.1849,  6.3306,  5.3873,  4.4283]]],\n",
       " \n",
       " \n",
       "         [[[ 0.9756,  2.4390, -0.2683, -0.6301, -0.6301, -0.4098, -2.3707,\n",
       "            -3.9382, -5.2610, -6.1951, -6.4398],\n",
       "           [ 0.5395,  1.0299, -0.0981, -0.5366, -0.5366,  0.4806, -1.4321,\n",
       "            -3.0799, -4.2962, -5.3163, -5.6384],\n",
       "           [-0.0489,  0.7828, -1.1742,  0.2452,  0.2452,  0.5039, -1.1448,\n",
       "            -2.7479, -4.0374, -5.1703, -5.5855]]]], device='cuda:0'),\n",
       " 'query_labels': tensor([[ 4.1296],\n",
       "         [-5.0147]], device='cuda:0'),\n",
       " 'support': tensor([[[[ 0.9756,  2.4390, -0.2683, -0.6301, -0.6301, -0.4098, -2.3707,\n",
       "            -3.9382, -5.2610, -6.1951, -6.4398],\n",
       "           [ 0.5395,  1.0299, -0.0981, -0.5366, -0.5366,  0.4806, -1.4321,\n",
       "            -3.0799, -4.2962, -5.3163, -5.6384],\n",
       "           [-0.0489,  0.7828, -1.1742,  0.2452,  0.2452,  0.5039, -1.1448,\n",
       "            -2.7479, -4.0374, -5.1703, -5.5855]],\n",
       " \n",
       "          [[ 0.5395,  1.0299, -0.0981, -0.5366, -0.5366,  0.4806, -1.4321,\n",
       "            -3.0799, -4.2962, -5.3163, -5.6384],\n",
       "           [-0.0489,  0.7828, -1.1742,  0.2452,  0.2452,  0.5039, -1.1448,\n",
       "            -2.7479, -4.0374, -5.1703, -5.5855],\n",
       "           [ 3.5024,  4.4038, -0.1288, -5.0147, -5.0147,  4.4296,  4.0690,\n",
       "             2.6732,  1.2091,  0.0299, -0.5468]],\n",
       " \n",
       "          [[-0.0489,  0.7828, -1.1742,  0.2452,  0.2452,  0.5039, -1.1448,\n",
       "            -2.7479, -4.0374, -5.1703, -5.5855],\n",
       "           [ 3.5024,  4.4038, -0.1288, -5.0147, -5.0147,  4.4296,  4.0690,\n",
       "             2.6732,  1.2091,  0.0299, -0.5468],\n",
       "           [-0.5876,  1.1242, -2.7082,  0.7983,  0.7983,  2.5192,  3.2013,\n",
       "             2.0082,  0.6745, -0.4854, -1.2936]],\n",
       " \n",
       "          [[ 3.5024,  4.4038, -0.1288, -5.0147, -5.0147,  4.4296,  4.0690,\n",
       "             2.6732,  1.2091,  0.0299, -0.5468],\n",
       "           [-0.5876,  1.1242, -2.7082,  0.7983,  0.7983,  2.5192,  3.2013,\n",
       "             2.0082,  0.6745, -0.4854, -1.2936],\n",
       "           [ 1.2098,  1.2098, -2.0592, -0.7409, -0.7409,  2.1776,  3.6396,\n",
       "             2.7473,  1.5779,  0.4211, -0.4736]],\n",
       " \n",
       "          [[-0.5876,  1.1242, -2.7082,  0.7983,  0.7983,  2.5192,  3.2013,\n",
       "             2.0082,  0.6745, -0.4854, -1.2936],\n",
       "           [ 1.2098,  1.2098, -2.0592, -0.7409, -0.7409,  2.1776,  3.6396,\n",
       "             2.7473,  1.5779,  0.4211, -0.4736],\n",
       "           [ 1.6715,  3.1308,  0.0000, -2.9858, -2.9858,  3.6827,  6.2006,\n",
       "             5.6602,  4.5702,  3.5765,  2.6523]]],\n",
       " \n",
       " \n",
       "         [[[-2.2671,  0.3488, -2.4913,  2.1114,  2.1114, -2.2820, -3.6821,\n",
       "            -4.8829, -5.6627, -5.6672, -5.5074],\n",
       "           [ 0.1997,  0.6490, -0.4993, -0.1993, -0.1993, -1.6076, -3.1503,\n",
       "            -4.1970, -5.0899, -5.1882, -5.2055],\n",
       "           [-1.0662,  0.2727, -1.1406,  0.6740,  0.6740, -1.4679, -3.0697,\n",
       "            -4.2632, -5.3149, -5.5324, -5.7220]],\n",
       " \n",
       "          [[ 0.1997,  0.6490, -0.4993, -0.1993, -0.1993, -1.6076, -3.1503,\n",
       "            -4.1970, -5.0899, -5.1882, -5.2055],\n",
       "           [-1.0662,  0.2727, -1.1406,  0.6740,  0.6740, -1.4679, -3.0697,\n",
       "            -4.2632, -5.3149, -5.5324, -5.7220],\n",
       "           [-3.3245,  0.5541, -3.6377,  2.9259,  2.9259, -2.9872, -4.7772,\n",
       "            -6.3294, -7.4572, -7.8988, -8.1498]],\n",
       " \n",
       "          [[-1.0662,  0.2727, -1.1406,  0.6740,  0.6740, -1.4679, -3.0697,\n",
       "            -4.2632, -5.3149, -5.5324, -5.7220],\n",
       "           [-3.3245,  0.5541, -3.6377,  2.9259,  2.9259, -2.9872, -4.7772,\n",
       "            -6.3294, -7.4572, -7.8988, -8.1498],\n",
       "           [-0.1697,  0.1454, -0.8240, -0.6023, -0.6023, -1.4542, -3.4755,\n",
       "            -5.0816, -6.3112, -7.0887, -7.2895]],\n",
       " \n",
       "          [[-3.3245,  0.5541, -3.6377,  2.9259,  2.9259, -2.9872, -4.7772,\n",
       "            -6.3294, -7.4572, -7.8988, -8.1498],\n",
       "           [-0.1697,  0.1454, -0.8240, -0.6023, -0.6023, -1.4542, -3.4755,\n",
       "            -5.0816, -6.3112, -7.0887, -7.2895],\n",
       "           [ 0.9756,  2.4390, -0.2683, -0.6301, -0.6301, -0.4098, -2.3707,\n",
       "            -3.9382, -5.2610, -6.1951, -6.4398]],\n",
       " \n",
       "          [[-0.1697,  0.1454, -0.8240, -0.6023, -0.6023, -1.4542, -3.4755,\n",
       "            -5.0816, -6.3112, -7.0887, -7.2895],\n",
       "           [ 0.9756,  2.4390, -0.2683, -0.6301, -0.6301, -0.4098, -2.3707,\n",
       "            -3.9382, -5.2610, -6.1951, -6.4398],\n",
       "           [ 0.5395,  1.0299, -0.0981, -0.5366, -0.5366,  0.4806, -1.4321,\n",
       "            -3.0799, -4.2962, -5.3163, -5.6384]]]], device='cuda:0'),\n",
       " 'support_labels': tensor([[-5.0147,  0.7983, -0.7409, -2.9858, -1.6981],\n",
       "         [ 2.9259, -0.6023, -0.6301, -0.5366,  0.2452]], device='cuda:0')}"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a2b19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to(device: None | str=None):\n",
    "        if device is None:\n",
    "            device = torch.device('cpu')\n",
    "        else:\n",
    "            device = torch.device(device)\n",
    "\n",
    "        for key in all_tasks.keys():\n",
    "            tvalue = tensor_fn(value, key)\n",
    "            self.__setitem__(key, tvalue.to(device)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e7105ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3, 11)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tasks['support'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4b9e45f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 11)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tasks['query'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f50810db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_spt_task:[[[0, 1, 2], [1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6]], [[1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6], [5, 6, 7]]]\n",
      "y_spt_task: [[3, 4, 5, 6, 7], [4, 5, 6, 7, 8]]\n"
     ]
    }
   ],
   "source": [
    "print(f'x_spt_task:{x_spt_task[:2]}')\n",
    "print(f'y_spt_task: {y_spt_task[:2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d9c3de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_spt_task[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "340c916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_inputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "22e3a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_inputs.append(df_single.iloc[x_spt_task[0][0]].to_numpy()[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0e3bf60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 11)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(support_inputs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c3b1b8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "24994136",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_labels.append(df_single['close'].iloc[y_spt_task[0]].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "096a8356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.26134388,  3.84167883,  0.34753364, -0.49156072, -0.62871896]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(support_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "14cc7d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(support_labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4673279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_qry_task:[[[5, 6, 7]], [[6, 7, 8]]]\n",
      "y_qry_task: [[8], [9]]\n"
     ]
    }
   ],
   "source": [
    "print(f'x_qry_task:{x_qry_task[:2]}')\n",
    "print(f'y_qry_task: {y_qry_task[:2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3f78c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "i,window_size = 0,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c6c6daf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spt_idx = [idx for idx in range(i, i+window_size)]\n",
    "spt_label_idx =  [spt_idx[-1] + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "efcac814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4], [5])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spt_idx, spt_label_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bcf1efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_df = 20\n",
    "window_size = 2\n",
    "n_support = 3\n",
    "n_query = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a1a8e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_spt_task = []\n",
    "y_spt_task = []\n",
    "x_qry_task = []\n",
    "y_qry_task = []\n",
    "\n",
    "for i in range(len_df-window_size-n_support-n_query+1):\n",
    "    x_spt = []\n",
    "    y_spt = []\n",
    "    x_qry = []\n",
    "    y_qry = []\n",
    "    \n",
    "    for j in range(n_support+n_query):\n",
    "        if j < n_support:\n",
    "            spt_idx = [idx for idx in range(i+j, i+j+window_size)]\n",
    "            x_spt.append(spt_idx)\n",
    "            y_spt.append(i+j+window_size)\n",
    "           \n",
    "        else:\n",
    "            qry_idx = [idx for idx in range(i+j, i+j+window_size)]\n",
    "            x_qry.append(qry_idx)\n",
    "            y_qry.append(i+j+window_size)\n",
    "\n",
    "    x_spt_task.append(x_spt)\n",
    "    y_spt_task.append(y_spt)\n",
    "    x_qry_task.append(x_qry)\n",
    "    y_qry_task.append(y_qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a92eb363",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0, 1], [1, 2], [2, 3]],\n",
       " [[1, 2], [2, 3], [3, 4]],\n",
       " [[2, 3], [3, 4], [4, 5]],\n",
       " [[3, 4], [4, 5], [5, 6]],\n",
       " [[4, 5], [5, 6], [6, 7]],\n",
       " [[5, 6], [6, 7], [7, 8]],\n",
       " [[6, 7], [7, 8], [8, 9]],\n",
       " [[7, 8], [8, 9], [9, 10]],\n",
       " [[8, 9], [9, 10], [10, 11]],\n",
       " [[9, 10], [10, 11], [11, 12]],\n",
       " [[10, 11], [11, 12], [12, 13]],\n",
       " [[11, 12], [12, 13], [13, 14]],\n",
       " [[12, 13], [13, 14], [14, 15]],\n",
       " [[13, 14], [14, 15], [15, 16]]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_spt_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6162a226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3, 4],\n",
       " [3, 4, 5],\n",
       " [4, 5, 6],\n",
       " [5, 6, 7],\n",
       " [6, 7, 8],\n",
       " [7, 8, 9],\n",
       " [8, 9, 10],\n",
       " [9, 10, 11],\n",
       " [10, 11, 12],\n",
       " [11, 12, 13],\n",
       " [12, 13, 14],\n",
       " [13, 14, 15],\n",
       " [14, 15, 16],\n",
       " [15, 16, 17]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_spt_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9580efd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[3, 4], [4, 5]],\n",
       " [[4, 5], [5, 6]],\n",
       " [[5, 6], [6, 7]],\n",
       " [[6, 7], [7, 8]],\n",
       " [[7, 8], [8, 9]],\n",
       " [[8, 9], [9, 10]],\n",
       " [[9, 10], [10, 11]],\n",
       " [[10, 11], [11, 12]],\n",
       " [[11, 12], [12, 13]],\n",
       " [[12, 13], [13, 14]],\n",
       " [[13, 14], [14, 15]],\n",
       " [[14, 15], [15, 16]],\n",
       " [[15, 16], [16, 17]],\n",
       " [[16, 17], [17, 18]]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_qry_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e8539c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800d9a72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
